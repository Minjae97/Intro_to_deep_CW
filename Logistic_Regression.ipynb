{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tarfile\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "from numpy.random import rand\n",
    "from numpy import log, exp, matmul\n",
    "from numpy.random import default_rng"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainxs = np.load(\"../comp0090_assignment_1_data/fashion-train-imgs.npz\")\n",
    "trainys = np.load(\"../comp0090_assignment_1_data/fashion-train-labels.npz\")\n",
    "devxs   = np.load(\"../comp0090_assignment_1_data/fashion-dev-imgs.npz\")\n",
    "devys   = np.load(\"../comp0090_assignment_1_data/fashion-dev-labels.npz\")\n",
    "testxs  = np.load(\"../comp0090_assignment_1_data/fashion-test-imgs.npz\")\n",
    "testys  = np.load(\"../comp0090_assignment_1_data/fashion-test-labels.npz\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x2bec688bee0>"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2)\n",
    "fig.suptitle('Fashion MNIST')\n",
    "ax1.imshow(trainxs[1:,:,0])\n",
    "ax2.imshow(trainxs[:,:,7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "def showResults(epochs, loss_train, loss_val, acc_train, acc_val, max_val_acc_epoch):\n",
    "    print('The accuracy on the training and validation set, for the epoch on which we get the highest accuracy on the validation set is: {} / {} at epoch {}.'.format(acc_train[max_val_acc_epoch], acc_val[max_val_acc_epoch], max_val_acc_epoch+1))   \n",
    "\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    plt.suptitle(\"Loss and Accuracy Results of Training and Validation set\", fontsize = 16)\n",
    "\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(range(epochs), loss_train, label=\"Training Loss\")\n",
    "    plt.plot(range(epochs), loss_val, label=\"Validation Loss\")\n",
    "    plt.xlabel('Epoch', fontsize = 13)\n",
    "    plt.ylabel('Loss', fontsize = 13)\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(range(epochs), acc_train, label=\"Training Accuracy\")\n",
    "    plt.plot(range(epochs), acc_val, label=\"Validation Accuracy\")\n",
    "    plt.xlabel('Epoch', fontsize = 13)\n",
    "    plt.ylabel('Accuracy', fontsize = 13)\n",
    "    plt.legend()\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Loss on Training / Validation Data at Epoch 1: 0.1043 / 0.1063\n",
      "- Accuracy on Training / Validation Data at Epoch 1: 0.6776666666666666 / 0.682\n",
      "\n",
      "- Loss on Training / Validation Data at Epoch 2: 0.1020 / 0.1040\n",
      "- Accuracy on Training / Validation Data at Epoch 2: 0.6865833333333333 / 0.689\n",
      "\n",
      "- Loss on Training / Validation Data at Epoch 3: 0.0999 / 0.1018\n",
      "- Accuracy on Training / Validation Data at Epoch 3: 0.6945833333333333 / 0.701\n",
      "\n",
      "- Loss on Training / Validation Data at Epoch 4: 0.0979 / 0.0997\n",
      "- Accuracy on Training / Validation Data at Epoch 4: 0.7030833333333333 / 0.71\n",
      "\n",
      "- Loss on Training / Validation Data at Epoch 5: 0.0960 / 0.0978\n",
      "- Accuracy on Training / Validation Data at Epoch 5: 0.70925 / 0.716\n",
      "\n",
      "- Loss on Training / Validation Data at Epoch 6: 0.0942 / 0.0960\n",
      "- Accuracy on Training / Validation Data at Epoch 6: 0.7165833333333333 / 0.722\n",
      "\n",
      "- Loss on Training / Validation Data at Epoch 7: 0.0924 / 0.0942\n",
      "- Accuracy on Training / Validation Data at Epoch 7: 0.723 / 0.723\n",
      "\n",
      "- Loss on Training / Validation Data at Epoch 8: 0.0908 / 0.0926\n",
      "- Accuracy on Training / Validation Data at Epoch 8: 0.729 / 0.732\n",
      "\n",
      "- Loss on Training / Validation Data at Epoch 9: 0.0893 / 0.0910\n",
      "- Accuracy on Training / Validation Data at Epoch 9: 0.7349166666666667 / 0.735\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-248-429c54fcb994>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     70\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     71\u001b[0m     \u001b[0midx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mshuffleIdx\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrainxs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 72\u001b[1;33m     \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrainxs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     73\u001b[0m     \u001b[0mY\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrainys\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Mon Nov  2 20:36:43 2020\n",
    "\n",
    "@author: Pavlos\n",
    "\"\"\"\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "from numpy.random import rand\n",
    "from numpy import exp, matmul\n",
    "from numpy.random import default_rng\n",
    "\n",
    "\n",
    "def loadData(filename):\n",
    "    file = os.path.join(os.getcwd(), os.pardir, \"comp0090_assignment_1_data\", filename)\n",
    "    return np.load(file)\n",
    "\n",
    "def shuffleIdx(n):\n",
    "    rng = default_rng()\n",
    "    rand_idx = rng.permutation(n)\n",
    "    return rand_idx\n",
    "\n",
    "def initialiseMLP():\n",
    "    mlp_params = {'W': np.random.rand(inputPar,1)/100,\n",
    "           'b': np.zeros((1, 1))}\n",
    "\n",
    "    keys = iter(mlp_params)\n",
    "    layers_sizes = [inputPar, 1]\n",
    "    len(layers_sizes)\n",
    "    for s in range(len(layers_sizes)-1):\n",
    "        key = next(keys)\n",
    "        epsilon = 4.0 * np.sqrt(6) / np.sqrt(layers_sizes[s] + layers_sizes[s+1])\n",
    "        mlp_params[key] = epsilon * ( (rand(layers_sizes[s+1], layers_sizes[s]) * 2.0 ) - 1)\n",
    "    \n",
    "    return mlp_params\n",
    "\n",
    "\n",
    "trainxs = loadData(\"fashion-train-imgs.npz\").reshape(784, 1, -1)\n",
    "trainys = loadData(\"fashion-train-labels.npz\")\n",
    "devxs   = loadData(\"fashion-dev-imgs.npz\").reshape(784, 1, -1)\n",
    "devys   = loadData(\"fashion-dev-labels.npz\")\n",
    "\n",
    "\n",
    "############################IMPORTANT PARAMETERS#############################\n",
    "epochs=600\n",
    "inputPar = trainxs.shape[0] * trainxs.shape[1]\n",
    "###################################\n",
    "\n",
    "mlp_params = initialiseMLP()\n",
    "grad_mlp = copy.deepcopy(mlp_params)\n",
    "\n",
    "sig = lambda x : 1/(1 + exp(-x))\n",
    "sigPrime = lambda x : sig(x)*(1 - sig(x))\n",
    "a1 = lambda m, x : matmul(m['W'], x)[0,0] + m['b'][0,0]\n",
    "f = h1 = lambda m, x : sig(a1(m,x))\n",
    "\n",
    "########### LEARNING PROCESS ################\n",
    "lr = 0.1\n",
    "losslist_train = []\n",
    "losslist_val = []\n",
    "acclist_train = []\n",
    "acclist_val = []\n",
    "\n",
    "max_val_acc_epoch = 0\n",
    "n_samples = trainxs.shape[2]\n",
    "n_samples_val = devxs.shape[2]\n",
    "\n",
    "for e in range(epochs):\n",
    "    idx = shuffleIdx(trainxs.shape[2])\n",
    "    X = trainxs[:, :, idx]\n",
    "    Y = trainys[idx]\n",
    "    \n",
    "    loss_train = 0\n",
    "    loss_val = 0\n",
    "    \n",
    "    acc_train = 0\n",
    "    train_correct = 0\n",
    "    val_correct = 0\n",
    "    cnt = 1\n",
    "    \n",
    "    total_grad = {'W': np.zeros((1, inputPar)),\n",
    "             'b': np.zeros((1, 1))}\n",
    "    \n",
    "    # UPDATE GRADIENTS FOR EACH IMAGE IN THE TRAINING DATASET\n",
    "    for i in range(trainxs.shape[2]): #number of data\n",
    "        x = X[:, :, i]\n",
    "        y = Y[i]\n",
    "        \n",
    "        prediction = f(mlp_params,x)\n",
    "        \n",
    "        if prediction >= 0.5:\n",
    "            yprime = 1\n",
    "        else:\n",
    "            yprime = 0\n",
    "            \n",
    "        if yprime == y:\n",
    "            train_correct += 1\n",
    "            \n",
    "        \n",
    "        loss_train += ((y - prediction)**2)/2\n",
    "        \n",
    "        grad_mlp['b'][0,0] = -(y-prediction)*(prediction)*(1-prediction)\n",
    "        grad_mlp['W'] = grad_mlp['b'][0,0] * x\n",
    "        \n",
    "        # SUM THE GRADIENTS OF X\n",
    "        for key in mlp_params:\n",
    "            total_grad[key] += grad_mlp[key].T\n",
    "            \n",
    "                \n",
    "    # COMPUTING LOSS AND ACCURACY OF VALIDATION SET\n",
    "        if (i < devxs.shape[2]):\n",
    "            val_x = devxs[:, :, i]\n",
    "            val_y = devys[i]\n",
    "            \n",
    "            prediction = f(mlp_params,val_x)\n",
    "            if prediction  >= 0.5:\n",
    "                yprime = 1\n",
    "            else:\n",
    "                yprime = 0\n",
    "\n",
    "            if yprime == val_y:\n",
    "                val_correct += 1\n",
    "                \n",
    "            loss_val += ((val_y - prediction)**2)/2\n",
    "\n",
    "\n",
    "    # COMPUTING THE AVERAGE OF GRADIENTS FOR EACH EPOCH SINCE WE ARE DOING        \n",
    "    # FULL-BATCH GRADIENT DESCEND AND UPDATING THE PARAMETERS AFTER EACH EPOCH\n",
    "    for key in total_grad:\n",
    "        total_grad[key] = total_grad[key]/n_samples\n",
    "        mlp_params[key] -= lr * total_grad[key]\n",
    "    \n",
    "    loss_train = loss_train/n_samples\n",
    "    losslist_train.append(loss_train)\n",
    "    \n",
    "    loss_val = loss_val/(n_samples_val)\n",
    "    losslist_val.append(loss_val)\n",
    "    \n",
    "    train_correct = train_correct/n_samples\n",
    "    acclist_train.append(train_correct)\n",
    "    \n",
    "    val_correct = val_correct/(n_samples_val)\n",
    "    acclist_val.append(val_correct)\n",
    "    \n",
    "\n",
    "\n",
    "    if  val_correct > acclist_val[max_val_acc_epoch]:\n",
    "        max_val_acc_epoch = e\n",
    "\n",
    "    print(\"- Loss on Training / Validation Data at Epoch {}: {:.4f} / {:.4f}\".format(e+1, loss_train, loss_val)) \n",
    "    print(\"- Accuracy on Training / Validation Data at Epoch {}: {} / {}\\n\".format(e+1, train_correct, val_correct))\n",
    "\n",
    "def showResults(epochs, loss_train, loss_val, acc_train, acc_val, max_val_acc_epoch):\n",
    "    print('The accuracy on the training and validation set, for the epoch on which we get the highest accuracy on the validation set is: {} / {} at epoch {}.'.format(acc_train[max_val_acc_epoch], acc_val[max_val_acc_epoch], max_val_acc_epoch+1))   \n",
    "\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    plt.suptitle(\"Loss and Accuracy Results of Training and Validation set\", fontsize = 16)\n",
    "\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(range(epochs), loss_train, label=\"Training Loss\")\n",
    "    plt.plot(range(epochs), loss_val, label=\"Validation Loss\")\n",
    "    plt.xlabel('Epoch', fontsize = 13)\n",
    "    plt.ylabel('Loss', fontsize = 13)\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(range(epochs), acc_train, label=\"Training Accuracy\")\n",
    "    plt.plot(range(epochs), acc_val, label=\"Validation Accuracy\")\n",
    "    plt.xlabel('Epoch', fontsize = 13)\n",
    "    plt.ylabel('Accuracy', fontsize = 13)\n",
    "    plt.legend()\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "showResults(epochs, losslist_train, losslist_val, acclist_train,acclist_val, max_val_acc_epoch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "sig = lambda x : 1/(1 + exp(-x))\n",
    "sigPrime = lambda x : sig(x)*(1 - sig(x))\n",
    "a1 = lambda m, x : matmul(m['W'], x)[0,0] + m['b'][0,0]\n",
    "f = h1 = lambda m, x : sig(a1(m,x))\n",
    "\n",
    "def checkGradient(mlp_params, X, Y):\n",
    "    # COMPARE AND CHECK ANALYTICAL GRADIENT WITH FINITE DIFFERENCE GRADIENT OF PARAMETERS\n",
    "    for s in range(X.shape[2]):\n",
    "        print(\"\\n- Sample \", s+1)\n",
    "        x = X[:,:,s].reshape(784, 1);\n",
    "        grad_mlp = copy.deepcopy(mlp_params)\n",
    "        prediction = f(mlp_params,x)\n",
    "        grad_mlp['b'][0,0] = -(y-prediction)*(prediction)*(1-prediction)\n",
    "        grad_mlp['W'] = grad_mlp['b'][0,0] * x.T\n",
    "        inputPar = trainxs.shape[0] * trainxs.shape[1]\n",
    "\n",
    "        fdgrad_mlp = copy.deepcopy(mlp_params)\n",
    "        diff_grad_mlp = copy.deepcopy(mlp_params)\n",
    "        check_mlp = {'W': False, 'b': False}\n",
    "\n",
    "        # COMPUTE FINITE DIFFERENCE GRADIENT OF PARAMETERS\n",
    "        eps = 0.00001\n",
    "\n",
    "        for key, par in mlp_params.items():\n",
    "            for i in range(par.shape[0]):\n",
    "                for j in range(par.shape[1]):\n",
    "                    temp = par[i,j]\n",
    "                    par[i,j] += eps/2\n",
    "                    r = ((Y[s] - f(mlp_params,x))**2)/2\n",
    "                    par[i,j] = temp\n",
    "                    par[i,j] -= eps/2\n",
    "                    l = ((Y[s] - f(mlp_params,x))**2)/2\n",
    "                    par[i,j] = temp\n",
    "\n",
    "                    fdgrad_mlp[key][i,j] = (r-l)/eps\n",
    "                    diff_grad_mlp[key][i,j] = abs(fdgrad_mlp[key][i,j] - grad_mlp[key][i,j])\n",
    "\n",
    "            check_mlp[key] = (diff_grad_mlp[key] < 1e-6).all()\n",
    "\n",
    "\n",
    "        print(\"\\nCheck if the difference between Analytical Gradient and Finite Difference Gradient of parameters is below 1e-6 threshold:\")\n",
    "        print(check_mlp, \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "- Sample  1\n",
      "\n",
      "Check if the difference between Analytical Gradient and Finite Difference Gradient of parameters is below 1e-6 threshold:\n",
      "{'W': True, 'b': True} \n",
      "\n",
      "\n",
      "- Sample  2\n",
      "\n",
      "Check if the difference between Analytical Gradient and Finite Difference Gradient of parameters is below 1e-6 threshold:\n",
      "{'W': True, 'b': True} \n",
      "\n"
     ]
    }
   ],
   "source": [
    "mlp_params = initialiseMLP()\n",
    "checkGradient(mlp_params, trainxs[:,:,[300,1000]], trainys[[300,1000]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
